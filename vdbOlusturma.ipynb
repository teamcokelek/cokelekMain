{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EVyamH3cK-NI",
    "outputId": "202cf466-f988-4a8c-c39f-1ee13b4c521c"
   },
   "outputs": [],
   "source": [
    "!unzip /content/ozetler.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qjDpW0H6MFld",
    "outputId": "2f91dec9-c07d-415b-86c8-ef9647d5b8b5"
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers faiss-cpu numpy sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pIIyTMHgk2dP",
    "outputId": "b8d99c7c-3caa-4a30-bc9e-c9829f6d5124"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "import sqlite3\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "\n",
    "SUMMARIES_FOLDER = '/content/summaries'\n",
    "DB_PATH = '/content/movies_final.db'\n",
    "INDEX_FILE = 'movie_summaries_chunked.index'\n",
    "CHUNK_MAP_FILE = 'chunk_map.pkl' \n",
    "CHUNK_SIZE = 300 \n",
    "CHUNK_OVERLAP = 50  \n",
    "\n",
    "def chunk_text_fixed_size(text, chunk_size=300, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        if len(chunk.strip()) > 10:\n",
    "            chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def chunk_text_semantic(text, max_chunk_size=400):\n",
    "    sentences = re.split(r'[.!?…]+', text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "\n",
    "        sentence_length = len(sentence.split())\n",
    "\n",
    "        if current_length + sentence_length > max_chunk_size and current_chunk:\n",
    "            chunks.append('. '.join(current_chunk) + '.')\n",
    "            current_chunk = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append('. '.join(current_chunk) + '.')\n",
    "\n",
    "    return chunks\n",
    "\n",
    "print(\"Embedding modeli (emrecan/bert-base-turkish...) yükleniyor...\")\n",
    "model = SentenceTransformer('emrecan/bert-base-turkish-cased-mean-nli-stsb-tr')\n",
    "print(\"Model başarıyla yüklendi.\")\n",
    "\n",
    "try:\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT title FROM movies\")\n",
    "    db_titles = [row[0] for row in cursor.fetchall()]\n",
    "    conn.close()\n",
    "    print(f\"SQL veritabanından {len(db_titles)} adet film başlığı okundu.\")\n",
    "except Exception as e:\n",
    "    print(f\"HATA: Veritabanı okunurken bir sorun oluştu: {e}\")\n",
    "    db_titles = []\n",
    "\n",
    "all_embeddings = []\n",
    "chunk_metadata = [] \n",
    "\n",
    "if db_titles:\n",
    "    print(f\"'{SUMMARIES_FOLDER}' klasöründeki özetler chunking ile işleniyor...\")\n",
    "\n",
    "    total_chunks = 0\n",
    "    processed_movies = 0\n",
    "\n",
    "    for title in db_titles:\n",
    "        safe_filename = re.sub(r'[\\\\/*?:\"<>|]', \"_\", title)\n",
    "        file_path = os.path.join(SUMMARIES_FOLDER, f\"{safe_filename}.txt\")\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    summary_text = f.read().strip()\n",
    "\n",
    "                if summary_text:\n",
    "                    chunks = chunk_text_semantic(summary_text, max_chunk_size=CHUNK_SIZE)\n",
    "\n",
    "                    for chunk_idx, chunk in enumerate(chunks):\n",
    "                        if len(chunk.strip()) > 20:\n",
    "                            try:\n",
    "                                embedding = model.encode(chunk, convert_to_tensor=False)\n",
    "                                all_embeddings.append(embedding)\n",
    "\n",
    "                                chunk_metadata.append({\n",
    "                                    'title': title,\n",
    "                                    'chunk_index': chunk_idx,\n",
    "                                    'chunk_text': chunk,\n",
    "                                    'total_chunks': len(chunks)\n",
    "                                })\n",
    "                                total_chunks += 1\n",
    "                            except Exception as e:\n",
    "                                print(f\"Chunk embedding hatası - {title} (chunk {chunk_idx}): {e}\")\n",
    "\n",
    "                    processed_movies += 1\n",
    "                    if processed_movies % 10 == 0:\n",
    "                        print(f\"İşlenen film sayısı: {processed_movies}/{len(db_titles)}, Toplam chunk: {total_chunks}\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"Uyarı: {safe_filename}.txt dosyası boş, atlanıyor.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Hata: {title}.txt dosyası işlenirken sorun oluştu: {e}\")\n",
    "        else:\n",
    "            print(f\"Uyarı: '{safe_filename}.txt' için özet dosyası bulunamadı, atlanıyor.\")\n",
    "\n",
    "    if all_embeddings:\n",
    "        embeddings_np = np.array(all_embeddings, dtype='float32')\n",
    "        print(f\"\\n📊 Özet İstatistikler:\")\n",
    "        print(f\"   • İşlenen film sayısı: {processed_movies}\")\n",
    "        print(f\"   • Toplam chunk sayısı: {len(chunk_metadata)}\")\n",
    "        print(f\"   • Ortalama chunk/film: {len(chunk_metadata)/processed_movies:.1f}\")\n",
    "        print(f\"   • Embedding boyutu: {embeddings_np.shape[1]}\")\n",
    "\n",
    "        vector_dimension = embeddings_np.shape[1]\n",
    "        index = faiss.IndexFlatL2(vector_dimension)\n",
    "\n",
    "        print(f\"\\nFAISS indeksi {vector_dimension} boyutlu olarak oluşturuldu.\")\n",
    "\n",
    "        index.add(embeddings_np)\n",
    "        print(f\"{index.ntotal} chunk vektörü FAISS indeksine eklendi.\")\n",
    "\n",
    "        faiss.write_index(index, INDEX_FILE)\n",
    "        print(f\"FAISS indeksi '{INDEX_FILE}' dosyasına kaydedildi.\")\n",
    "\n",
    "        with open(CHUNK_MAP_FILE, 'wb') as f:\n",
    "            pickle.dump(chunk_metadata, f)\n",
    "        print(f\"Chunk metadata '{CHUNK_MAP_FILE}' dosyasına kaydedildi.\")\n",
    "\n",
    "        print(\"\\n✅ Chunking tabanlı vektör veritabanı oluşturma işlemi başarıyla tamamlandı!\")\n",
    "\n",
    "        if chunk_metadata:\n",
    "            print(f\"\\n📝 İlk chunk örneği ({chunk_metadata[0]['title']}):\")\n",
    "            print(f\"   Chunk {chunk_metadata[0]['chunk_index']+1}/{chunk_metadata[0]['total_chunks']}\")\n",
    "            print(f\"   İçerik (ilk 200 karakter): {chunk_metadata[0]['chunk_text'][:200]}...\")\n",
    "\n",
    "    else:\n",
    "        print(\"İşlenecek hiç geçerli chunk bulunamadı.\")\n",
    "else:\n",
    "    print(\"Veritabanından film başlığı okunamadığı için işlem durduruldu.\")\n",
    "\n",
    "def search_similar_chunks(query, top_k=5):\n",
    "    try:\n",
    "        index = faiss.read_index(INDEX_FILE)\n",
    "        with open(CHUNK_MAP_FILE, 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "\n",
    "        query_embedding = model.encode(query, convert_to_tensor=False)\n",
    "        query_embedding = np.array([query_embedding], dtype='float32')\n",
    "\n",
    "        distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "        results = []\n",
    "        for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "            if idx < len(metadata):\n",
    "                chunk_info = metadata[idx]\n",
    "                results.append({\n",
    "                    'rank': i + 1,\n",
    "                    'similarity_score': float(1 / (1 + dist)), \n",
    "                    'movie_title': chunk_info['title'],\n",
    "                    'chunk_index': chunk_info['chunk_index'],\n",
    "                    'total_chunks': chunk_info['total_chunks'],\n",
    "                    'chunk_text': chunk_info['chunk_text'],\n",
    "                    'distance': float(dist)\n",
    "                })\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Arama hatası: {e}\")\n",
    "        return []\n",
    "\n",
    "print(f\"\\n🔍 Test sorgusu çalıştırılıyor...\")\n",
    "test_query = \"romantik aşk hikayesi\"\n",
    "results = search_similar_chunks(test_query, top_k=3)\n",
    "\n",
    "if results:\n",
    "    print(f\"\\n'{test_query}' sorgusu için en benzer 3 chunk:\")\n",
    "    for result in results:\n",
    "        print(f\"\\n{result['rank']}. {result['movie_title']}\")\n",
    "        print(f\"   Chunk {result['chunk_index']+1}/{result['total_chunks']} (Benzerlik: {result['similarity_score']:.3f})\")\n",
    "        print(f\"   İçerik: {result['chunk_text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dfJAF07lFC2",
    "outputId": "57641aa9-b10c-4443-edc5-c837c794a7e3"
   },
   "outputs": [],
   "source": [
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def optimize_index_for_cosine():\n",
    "    index_l2 = faiss.read_index('movie_summaries_chunked.index')\n",
    "    with open('chunk_map.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "\n",
    "    print(f\"Mevcut indeks: {index_l2.ntotal} vektör, {index_l2.d} boyut\")\n",
    "\n",
    "    vectors = np.zeros((index_l2.ntotal, index_l2.d), dtype='float32')\n",
    "    for i in range(index_l2.ntotal):\n",
    "        vectors[i] = index_l2.reconstruct(i)\n",
    "\n",
    "    faiss.normalize_L2(vectors)\n",
    "\n",
    "    index_cosine = faiss.IndexFlatIP(index_l2.d) \n",
    "    index_cosine.add(vectors)\n",
    "\n",
    "    # Kaydet\n",
    "    faiss.write_index(index_cosine, 'movie_summaries_cosine.index')\n",
    "    print(f\"✅ Cosine similarity indeksi kaydedildi: {index_cosine.ntotal} vektör\")\n",
    "\n",
    "    return index_cosine\n",
    "\n",
    "def cosine_search(query, top_k=10, min_similarity=0.5):\n",
    "    try:\n",
    "        index = faiss.read_index('movie_summaries_cosine.index')\n",
    "        with open('chunk_map.pkl', 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "\n",
    "        query_vector = model.encode(query, convert_to_tensor=False)\n",
    "        query_vector = np.array([query_vector], dtype='float32')\n",
    "        faiss.normalize_L2(query_vector)\n",
    "\n",
    "        similarities, indices = index.search(query_vector, min(top_k * 2, len(metadata)))\n",
    "\n",
    "        results = []\n",
    "        for i, (sim, idx) in enumerate(zip(similarities[0], indices[0])):\n",
    "            if sim >= min_similarity and idx < len(metadata):\n",
    "                chunk_info = metadata[idx]\n",
    "                results.append({\n",
    "                    'rank': len(results) + 1,\n",
    "                    'similarity_score': float(sim),\n",
    "                    'movie_title': chunk_info['title'],\n",
    "                    'chunk_index': chunk_info['chunk_index'],\n",
    "                    'total_chunks': chunk_info['total_chunks'],\n",
    "                    'chunk_text': chunk_info['chunk_text'],\n",
    "                })\n",
    "\n",
    "        return results[:top_k]\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Cosine indeksi bulunamadı. optimize_index_for_cosine() fonksiyonunu çalıştırın.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Arama hatası: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"🔧 İndeks cosine similarity için optimize ediliyor...\")\n",
    "optimized_index = optimize_index_for_cosine()\n",
    "\n",
    "print(\"\\n🔍 Cosine similarity ile test...\")\n",
    "test_queries = [\n",
    "    \"pandemide evinde kalıp şarkılar çalan bi adam vardı komedyen\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n📝 Sorgu: '{query}'\")\n",
    "    results = cosine_search(query, top_k=5, min_similarity=0.3)\n",
    "\n",
    "    if results:\n",
    "        for result in results:\n",
    "            print(f\"   {result['rank']}. {result['movie_title']} (Skor: {result['similarity_score']:.3f})\")\n",
    "            print(f\"      Chunk {result['chunk_index']+1}: {result['chunk_text'][:100]}...\")\n",
    "    else:\n",
    "        print(\"   ❌ Uygun sonuç bulunamadı (min_similarity=0.3)\")\n",
    "\n",
    "print(\"\\n✅ Optimizasyon tamamlandı!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
